{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0203ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  \\\n",
      "0                                    b'Less Than Zero'   \n",
      "1    b'Women and Autoimmune Disease (The Mysterious...   \n",
      "2    b'In The Garden Of The North American Martyrs ...   \n",
      "3                     b\"Amelia Bedelia's Family Album\"   \n",
      "4    b'De la cabeza a los pies (From Head to Toe (S...   \n",
      "..                                                 ...   \n",
      "995  b'To Rule the Waves (How the British Navy Shap...   \n",
      "996            b\"Wonder When You'll Miss Me (A Novel)\"   \n",
      "997                                b'Whose Hat Is It?'   \n",
      "998                               b'All Shall Be Well'   \n",
      "999                         b'The Get Rich Quick Club'   \n",
      "\n",
      "                                                   Url   Price     ISBN-10  \\\n",
      "0    https://bulkbookstore.com/less-than-zero-97800...   $5.99  0060001267   \n",
      "1    https://bulkbookstore.com/women-and-autoimmune...  $16.99  0060081503   \n",
      "2    https://bulkbookstore.com/in-the-garden-of-the...  $13.95  0880014970   \n",
      "3    https://bulkbookstore.com/amelia-bedelias-fami...   $4.99  0060511168   \n",
      "4    https://bulkbookstore.com/from-head-to-toe-spa...   $8.99  0060513136   \n",
      "..                                                 ...     ...         ...   \n",
      "995  https://bulkbookstore.com/to-rule-the-waves-ho...  $19.99  0060534257   \n",
      "996  https://bulkbookstore.com/wonder-when-youll-mi...  $12.99  0060534265   \n",
      "997  https://bulkbookstore.com/whose-hat-is-it-9780...   $4.99  0060534362   \n",
      "998  https://bulkbookstore.com/all-shall-be-well-97...   $8.99  0060534397   \n",
      "999  https://bulkbookstore.com/the-get-rich-quick-c...   $6.99  0060534427   \n",
      "\n",
      "                 ISBN                              Author     Format  Pages  \\\n",
      "0    b'9780060001261'  Stuart J. Murphy, Frank Remkiewicz  Paperback   40.0   \n",
      "1    b'9780060081508'                    Robert G. Lahita  Paperback  304.0   \n",
      "2    b'9780880014977'                        Tobias Wolff  Paperback  192.0   \n",
      "3    b'9780060511166'            Peggy Parish, Lynn Sweat  Paperback   48.0   \n",
      "4    b'9780060513139'              Eric Carle, Eric Carle  Paperback   32.0   \n",
      "..                ...                                 ...        ...    ...   \n",
      "995  b'9780060534257'                       Arthur Herman  Paperback  688.0   \n",
      "996  b'9780060534264'                        Amanda Davis  Paperback  288.0   \n",
      "997  b'9780060534363'  Valeri Gorbachev, Valeri Gorbachev  Paperback   32.0   \n",
      "998  b'9780060534394'                     Deborah Crombie  Paperback  288.0   \n",
      "999  b'9780060534424'                          Dan Gutman  Paperback  128.0   \n",
      "\n",
      "                             Publisher Language  Weight  \\\n",
      "0      HarperCollins (August 14, 2003)  English  5.68oz   \n",
      "1         HarperCollins (July 5, 2005)  English  12.8oz   \n",
      "2         HarperCollins (May 11, 2004)  English   6.4oz   \n",
      "3    HarperCollins (February 18, 2003)  English  3.84oz   \n",
      "4      HarperCollins (January 9, 2007)  Spanish   6.4oz   \n",
      "..                                 ...      ...     ...   \n",
      "995   HarperCollins (October 25, 2005)  English    16oz   \n",
      "996     HarperCollins (March 16, 2004)  English  9.92oz   \n",
      "997       HarperCollins (May 24, 2005)  English  2.32oz   \n",
      "998  HarperCollins (February 24, 2004)  English  5.12oz   \n",
      "999   HarperCollins (October 17, 2006)  English    16oz   \n",
      "\n",
      "                Dimensions  Case pack  \\\n",
      "0                 10\" x 8\"        100   \n",
      "1          6\" x 9\" x 0.76\"         28   \n",
      "2     5.5\" x 8.25\" x 0.48\"         56   \n",
      "3          6\" x 9\" x 0.19\"         92   \n",
      "4         9\" x 12\" x 0.07\"         80   \n",
      "..                     ...        ...   \n",
      "995      5.31\" x 8\" x 1.1\"         28   \n",
      "996     5.31\" x 8\" x 0.65\"         22   \n",
      "997    5.5\" x 8.5\" x 0.12\"         92   \n",
      "998  4.19\" x 6.75\" x 0.72\"         48   \n",
      "999  5.12\" x 7.62\" x 0.26\"        132   \n",
      "\n",
      "                                       Raw description Availability  \\\n",
      "0    <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "1    <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "2    <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "3    <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "4    <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "..                                                 ...          ...   \n",
      "995  <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "996  <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "997  <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "998  <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "999  <article class=\"productView-description\">\\n<p ...      InStock   \n",
      "\n",
      "    Item condition                                        Breadcrumbs  \\\n",
      "0        Condition                           Home|Feed|Less Than Zero   \n",
      "1        Condition  Home|Medical|Women and Autoimmune Disease (The...   \n",
      "2        Condition  Home|Fiction|In The Garden Of The North Americ...   \n",
      "3        Condition  Home|Juvenile Fiction|Amelia Bedelia's Family ...   \n",
      "4        Condition  Home|Feed|De la cabeza a los pies (From Head t...   \n",
      "..             ...                                                ...   \n",
      "995      Condition  Home|History|To Rule the Waves (How the Britis...   \n",
      "996      Condition  Home|Fiction|Wonder When You'll Miss Me (A Novel)   \n",
      "997      Condition             Home|Juvenile Fiction|Whose Hat Is It?   \n",
      "998      Condition                     Home|Fiction|All Shall Be Well   \n",
      "999      Condition                  Home|Feed|The Get Rich Quick Club   \n",
      "\n",
      "                                                Images  \\\n",
      "0    https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "1    https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "2    https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "3    https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "4    https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "..                                                 ...   \n",
      "995  https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "996  https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "997  https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "998  https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "999  https://cdn11.bigcommerce.com/s-igquupw3/image...   \n",
      "\n",
      "                     Scraped at                               Uniq id  \n",
      "0    2021-11-27 02:29:46.353664  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "1    2021-11-27 02:29:48.740580  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "2    2021-11-27 02:29:50.330361  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "3    2021-11-27 02:29:52.082084  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "4    2021-11-27 02:29:53.668835  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "..                          ...                                   ...  \n",
      "995  2021-11-27 03:05:44.443001  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "996  2021-11-27 03:05:46.050596  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "997  2021-11-27 03:05:48.249406  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "998  2021-11-27 03:05:49.846117  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "999  2021-11-27 03:05:51.640448  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "\n",
      "[1000 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289aa96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Title              0\n",
      "Url                0\n",
      "Price              0\n",
      "ISBN-10            0\n",
      "ISBN               0\n",
      "Author             0\n",
      "Format             0\n",
      "Pages              4\n",
      "Publisher          0\n",
      "Language           0\n",
      "Weight             0\n",
      "Dimensions         8\n",
      "Case pack          0\n",
      "Raw description    0\n",
      "Availability       0\n",
      "Item condition     0\n",
      "Breadcrumbs        0\n",
      "Images             0\n",
      "Scraped at         0\n",
      "Uniq id            0\n",
      "dtype: int64\n",
      "Columns with text data:\n",
      "['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher', 'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', 'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
      "Title\n",
      "Url\n",
      "Price\n",
      "ISBN-10\n",
      "ISBN\n",
      "Author\n",
      "Format\n",
      "Publisher\n",
      "Language\n",
      "Weight\n",
      "Dimensions\n",
      "Raw description\n",
      "Availability\n",
      "Item condition\n",
      "Breadcrumbs\n",
      "Images\n",
      "Scraped at\n",
      "Uniq id\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "\n",
    "# CHECK MISSING VALUES\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# CCHECK DATA TYPE FOR EACH COL\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "#FILTER COL WITH STRING DATA TYPE \n",
    "text_columns = column_data_types[column_data_types == 'object'].index.tolist()\n",
    "\n",
    "\n",
    "print(\"Columns with text data:\")\n",
    "print(text_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for column in text_columns:\n",
    "    print(column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5cc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f229d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title Url    Price  \\\n",
      "0                                        b'less zero '       $ 5.99   \n",
      "1    b'women autoimmun diseas ( mysteri way bodi be...      $ 16.99   \n",
      "2        b'in garden north american martyr ( stori ) '      $ 13.95   \n",
      "3               b '' amelia bedelia 's famili album ''       $ 4.99   \n",
      "4    b'de la cabeza lo pie ( head toe ( spanish edi...       $ 8.99   \n",
      "..                                                 ...  ..      ...   \n",
      "995  b'to rule wave ( british navi shape modern wor...      $ 19.99   \n",
      "996                  b '' wonder 'll miss ( novel ) ''      $ 12.99   \n",
      "997                                    b'whose hat ? '       $ 4.99   \n",
      "998                                 b'all shall well '       $ 8.99   \n",
      "999                        b'the get rich quick club '       $ 6.99   \n",
      "\n",
      "        ISBN-10               ISBN                               Author  \\\n",
      "0    0060001267  b'9780060001261 '  stuart j. murphi , frank remkiewicz   \n",
      "1    0060081503  b'9780060081508 '                     robert g. lahita   \n",
      "2    0880014970  b'9780880014977 '                          tobia wolff   \n",
      "3    0060511168  b'9780060511166 '            peggi parish , lynn sweat   \n",
      "4    0060513136  b'9780060513139 '                eric carl , eric carl   \n",
      "..          ...                ...                                  ...   \n",
      "995  0060534257  b'9780060534257 '                        arthur herman   \n",
      "996  0060534265  b'9780060534264 '                          amanda davi   \n",
      "997  0060534362  b'9780060534363 '  valeri gorbachev , valeri gorbachev   \n",
      "998  0060534397  b'9780060534394 '                       deborah crombi   \n",
      "999  0060534427  b'9780060534424 '                           dan gutman   \n",
      "\n",
      "        Format  Pages                            Publisher Language  Weight  \\\n",
      "0    paperback   40.0    harpercollin ( august 14 , 2003 )  english  5.68oz   \n",
      "1    paperback  304.0       harpercollin ( juli 5 , 2005 )  english  12.8oz   \n",
      "2    paperback  192.0       harpercollin ( may 11 , 2004 )  english   6.4oz   \n",
      "3    paperback   48.0  harpercollin ( februari 18 , 2003 )  english  3.84oz   \n",
      "4    paperback   32.0    harpercollin ( januari 9 , 2007 )  spanish   6.4oz   \n",
      "..         ...    ...                                  ...      ...     ...   \n",
      "995  paperback  688.0     harpercollin ( octob 25 , 2005 )  english    16oz   \n",
      "996  paperback  288.0     harpercollin ( march 16 , 2004 )  english  9.92oz   \n",
      "997  paperback   32.0       harpercollin ( may 24 , 2005 )  english  2.32oz   \n",
      "998  paperback  288.0  harpercollin ( februari 24 , 2004 )  english  5.12oz   \n",
      "999  paperback  128.0     harpercollin ( octob 17 , 2006 )  english    16oz   \n",
      "\n",
      "                      Dimensions  Case pack  \\\n",
      "0                   10 '' x 8 ''        100   \n",
      "1          6 '' x 9 '' x 0.76 ''         28   \n",
      "2     5.5 '' x 8.25 '' x 0.48 ''         56   \n",
      "3          6 '' x 9 '' x 0.19 ''         92   \n",
      "4         9 '' x 12 '' x 0.07 ''         80   \n",
      "..                           ...        ...   \n",
      "995      5.31 '' x 8 '' x 1.1 ''         28   \n",
      "996     5.31 '' x 8 '' x 0.65 ''         22   \n",
      "997    5.5 '' x 8.5 '' x 0.12 ''         92   \n",
      "998  4.19 '' x 6.75 '' x 0.72 ''         48   \n",
      "999  5.12 '' x 7.62 '' x 0.26 ''        132   \n",
      "\n",
      "                                       Raw description Availability  \\\n",
      "0    < articl class= '' productview-descript '' > <...      instock   \n",
      "1    < articl class= '' productview-descript '' > <...      instock   \n",
      "2    < articl class= '' productview-descript '' > <...      instock   \n",
      "3    < articl class= '' productview-descript '' > <...      instock   \n",
      "4    < articl class= '' productview-descript '' > <...      instock   \n",
      "..                                                 ...          ...   \n",
      "995  < articl class= '' productview-descript '' > <...      instock   \n",
      "996  < articl class= '' productview-descript '' > <...      instock   \n",
      "997  < articl class= '' productview-descript '' > <...      instock   \n",
      "998  < articl class= '' productview-descript '' > <...      instock   \n",
      "999  < articl class= '' productview-descript '' > <...      instock   \n",
      "\n",
      "    Item condition                                        Breadcrumbs Images  \\\n",
      "0           condit                                home|feed|less zero          \n",
      "1           condit  home|medical|women autoimmun diseas ( mysteri ...          \n",
      "2           condit  home|fiction|in garden north american martyr (...          \n",
      "3           condit  home|juvenil fiction|amelia bedelia 's famili ...          \n",
      "4           condit  home|feed|d la cabeza lo pie ( head toe ( span...          \n",
      "..             ...                                                ...    ...   \n",
      "995         condit  home|history|to rule wave ( british navi shape...          \n",
      "996         condit               home|fiction|wond 'll miss ( novel )          \n",
      "997         condit                    home|juvenil fiction|whos hat ?          \n",
      "998         condit                         home|fiction|al shall well          \n",
      "999         condit                   home|feed|th get rich quick club          \n",
      "\n",
      "                     Scraped at                               Uniq id  \n",
      "0    2021-11-27 02:29:46.353664  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "1    2021-11-27 02:29:48.740580  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "2    2021-11-27 02:29:50.330361  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "3    2021-11-27 02:29:52.082084  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "4    2021-11-27 02:29:53.668835  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "..                          ...                                   ...  \n",
      "995  2021-11-27 03:05:44.443001  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "996  2021-11-27 03:05:46.050596  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "997  2021-11-27 03:05:48.249406  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "998  2021-11-27 03:05:49.846117  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "999  2021-11-27 03:05:51.640448  4ebd0208-8328-5d69-8c44-ec50939c0967  \n",
      "\n",
      "[1000 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "\n",
    "# REMOVE DUBLICATE ROWS\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# DROP ROWS WITH MISSING VALUES\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa704b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6425abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title Url    Price  \\\n",
      "0                                        b'less zero '       $ 5.99   \n",
      "1    b'women autoimmun diseas ( mysteri way bodi be...      $ 16.99   \n",
      "2        b'in garden north american martyr ( stori ) '      $ 13.95   \n",
      "3               b '' amelia bedelia 's famili album ''       $ 4.99   \n",
      "4    b'de la cabeza lo pie ( head toe ( spanish edi...       $ 8.99   \n",
      "..                                                 ...  ..      ...   \n",
      "995  b'to rule wave ( british navi shape modern wor...      $ 19.99   \n",
      "996                  b '' wonder 'll miss ( novel ) ''      $ 12.99   \n",
      "997                                    b'whose hat ? '       $ 4.99   \n",
      "998                                 b'all shall well '       $ 8.99   \n",
      "999                        b'the get rich quick club '       $ 6.99   \n",
      "\n",
      "        ISBN-10               ISBN                               Author  \\\n",
      "0    0060001267  b'9780060001261 '  stuart j. murphi , frank remkiewicz   \n",
      "1    0060081503  b'9780060081508 '                     robert g. lahita   \n",
      "2    0880014970  b'9780880014977 '                          tobia wolff   \n",
      "3    0060511168  b'9780060511166 '            peggi parish , lynn sweat   \n",
      "4    0060513136  b'9780060513139 '                eric carl , eric carl   \n",
      "..          ...                ...                                  ...   \n",
      "995  0060534257  b'9780060534257 '                        arthur herman   \n",
      "996  0060534265  b'9780060534264 '                          amanda davi   \n",
      "997  0060534362  b'9780060534363 '  valeri gorbachev , valeri gorbachev   \n",
      "998  0060534397  b'9780060534394 '                       deborah crombi   \n",
      "999  0060534427  b'9780060534424 '                           dan gutman   \n",
      "\n",
      "        Format  Pages                            Publisher Language  ...  \\\n",
      "0    paperback   40.0    harpercollin ( august 14 , 2003 )  english  ...   \n",
      "1    paperback  304.0       harpercollin ( juli 5 , 2005 )  english  ...   \n",
      "2    paperback  192.0       harpercollin ( may 11 , 2004 )  english  ...   \n",
      "3    paperback   48.0  harpercollin ( februari 18 , 2003 )  english  ...   \n",
      "4    paperback   32.0    harpercollin ( januari 9 , 2007 )  spanish  ...   \n",
      "..         ...    ...                                  ...      ...  ...   \n",
      "995  paperback  688.0     harpercollin ( octob 25 , 2005 )  english  ...   \n",
      "996  paperback  288.0     harpercollin ( march 16 , 2004 )  english  ...   \n",
      "997  paperback   32.0       harpercollin ( may 24 , 2005 )  english  ...   \n",
      "998  paperback  288.0  harpercollin ( februari 24 , 2004 )  english  ...   \n",
      "999  paperback  128.0     harpercollin ( octob 17 , 2006 )  english  ...   \n",
      "\n",
      "    ángele   él  éxito última única único ଯving   ೨e 埦rom   埨e  \n",
      "0      0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "1      0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "2      0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "..     ...  ...    ...    ...   ...   ...   ...  ...  ...  ...  \n",
      "995    0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "996    0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "997    0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "998    0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "999    0.0  0.0    0.0    0.0   0.0   0.0   0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1000 rows x 16235 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''  #RETURN AN EMTY STRING FOR NON-STRING OR NAN VALUES\n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "    \n",
    "\n",
    "#COMBINE PREPROCESSED TEXT COL INTO A SINGLE COL \n",
    "df['combined_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "#CONVERT TF-IDF MATRIX TO DATAFRAME \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE  TF-IDF DataFrame WITH THE ORGINAL DATAFRAME \n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b9bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e04a812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pages  Case pack   00  000  0007149824  0007154577  0007155360  \\\n",
      "0     40.0        100  0.0  0.0         0.0         0.0         0.0   \n",
      "1    304.0         28  0.0  0.0         0.0         0.0         0.0   \n",
      "2    192.0         56  0.0  0.0         0.0         0.0         0.0   \n",
      "3     48.0         92  0.0  0.0         0.0         0.0         0.0   \n",
      "4     32.0         80  0.0  0.0         0.0         0.0         0.0   \n",
      "..     ...        ...  ...  ...         ...         ...         ...   \n",
      "995  688.0         28  0.0  0.0         0.0         0.0         0.0   \n",
      "996  288.0         22  0.0  0.0         0.0         0.0         0.0   \n",
      "997   32.0         92  0.0  0.0         0.0         0.0         0.0   \n",
      "998  288.0         48  0.0  0.0         0.0         0.0         0.0   \n",
      "999  128.0        132  0.0  0.0         0.0         0.0         0.0   \n",
      "\n",
      "     0007155387  0007155425  000715612x  ...  ángele   él  éxito  última  \\\n",
      "0           0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "1           0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "2           0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "3           0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "4           0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "..          ...         ...         ...  ...     ...  ...    ...     ...   \n",
      "995         0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "996         0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "997         0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "998         0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "999         0.0         0.0         0.0  ...     0.0  0.0    0.0     0.0   \n",
      "\n",
      "     única  único  ଯving   ೨e  埦rom   埨e  \n",
      "0      0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "1      0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "2      0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "3      0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "4      0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "..     ...    ...    ...  ...   ...  ...  \n",
      "995    0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "996    0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "997    0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "998    0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "999    0.0    0.0    0.0  0.0   0.0  0.0  \n",
      "\n",
      "[1000 rows x 16216 columns]\n"
     ]
    }
   ],
   "source": [
    "#CREATE USER-ITEM MATRIX \n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''  \n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "#COMBINE PREPROCESSED TEXT COL INTO A SINGLE COL \n",
    "df['combined_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRIX TO DATAFRAME \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE TF-IDF DATAFRAME WITH THE ORGINAL DATAFRAME \n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# CRAETE USER-ITEM MATRIXS\n",
    "user_item_matrix = df.drop(columns=['combined_text', 'Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author',\n",
    "                                    'Format', 'Publisher', 'Language', 'Weight', 'Dimensions',\n",
    "                                    'Raw description', 'Availability', 'Item condition', \n",
    "                                    'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id'])\n",
    "\n",
    "\n",
    "print(user_item_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990aa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fcb7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Matrix:\n",
      "Book_ID  101  102  103\n",
      "User_ID               \n",
      "2          3    0    5\n",
      "3          0    4    2\n",
      "\n",
      "User Similarity Matrix:\n",
      "User_ID         2         3\n",
      "User_ID                    \n",
      "2        1.000000  0.383482\n",
      "3        0.383482  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''  \n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "#COMBINE PREPROCESSED TEXT COL INTO A SINGLE COL \n",
    "df['combined_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRIX TO DATAFRAME \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE TF-IDF DATAFRAME WITH THE ORGINAL DATAFRAME \n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# CRAETE USER-ITEM MATRIXS\n",
    "user_item_matrix = df.drop(columns=['combined_text', 'Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author',\n",
    "                                    'Format', 'Publisher', 'Language', 'Weight', 'Dimensions',\n",
    "                                    'Raw description', 'Availability', 'Item condition', \n",
    "                                    'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#DATAFRAME\n",
    "df = pd.DataFrame({\n",
    "    'User_ID': [1, 1, 2, 2, 3, 3],\n",
    "    'Book_ID': [101, 102, 101, 103, 102, 103],\n",
    "    'Rating': [5, 4, 3, 5, 4, 2]\n",
    "})\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND TEST \n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#USER ITEM MATRIX FOR TRAIN DATA \n",
    "train_user_item_matrix = train_data.pivot_table(index='User_ID', columns='Book_ID', values='Rating', fill_value=0)\n",
    "\n",
    "#CALCULAYE SIMILARITY \n",
    "user_similarity = cosine_similarity(train_user_item_matrix)\n",
    "\n",
    "\n",
    "print(\"User-Item Matrix:\")\n",
    "print(train_user_item_matrix)\n",
    "\n",
    "print(\"\\nUser Similarity Matrix:\")\n",
    "print(pd.DataFrame(user_similarity, index=train_user_item_matrix.index, columns=train_user_item_matrix.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fca8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d75544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 2.676782313273394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nh013\\AppData\\Local\\Temp\\ipykernel_9536\\2263173572.py:108: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  ratings_diff = (train_user_item_matrix - mean_user_rating[:, np.newaxis])\n",
      "C:\\Users\\nh013\\AppData\\Local\\Temp\\ipykernel_9536\\2263173572.py:109: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  pred = mean_user_rating[:, np.newaxis] + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''  \n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "#COMBINE PREPROCESSED TEXT COL INTO A SINGLE COL \n",
    "df['combined_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRIX TO DATAFRAME \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE TF-IDF DATAFRAME WITH THE ORGINAL DATAFRAME \n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# CRAETE USER-ITEM MATRIXS\n",
    "user_item_matrix = df.drop(columns=['combined_text', 'Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author',\n",
    "                                    'Format', 'Publisher', 'Language', 'Weight', 'Dimensions',\n",
    "                                    'Raw description', 'Availability', 'Item condition', \n",
    "                                    'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# DATAFRAME \n",
    "df = pd.DataFrame({\n",
    "    'User_ID': [1, 1, 2, 2, 3, 3],\n",
    "    'Book_ID': [101, 102, 101, 103, 102, 103],\n",
    "    'Rating': [5, 4, 3, 5, 4, 2]\n",
    "})\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND TEST SET\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# USER ITEM MATRIX FOR TRAIN DATA \n",
    "train_user_item_matrix = train_data.pivot_table(index='User_ID', columns='Book_ID', values='Rating', fill_value=0)\n",
    "\n",
    "# CALCULATE SIMILARITY \n",
    "user_similarity = cosine_similarity(train_user_item_matrix)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Assuming you already have 'test_data'\n",
    "# If not, replace it with your actual test data\n",
    "\n",
    "# PREDICT RATING FOR TEST DATA \n",
    "def predict_ratings(user_similarity, train_user_item_matrix):\n",
    "    mean_user_rating = train_user_item_matrix.mean(axis=1)\n",
    "    ratings_diff = (train_user_item_matrix - mean_user_rating[:, np.newaxis])\n",
    "    pred = mean_user_rating[:, np.newaxis] + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "    return pred\n",
    "\n",
    "\n",
    "test_user_item_matrix = test_data.pivot_table(index='User_ID', columns='Book_ID', values='Rating', fill_value=0)\n",
    "\n",
    "# GET PREDICTED RATING \n",
    "user_predictions = predict_ratings(user_similarity, train_user_item_matrix)\n",
    "\n",
    "# EVALUATE THE MODEL \n",
    "#FLATTEN THE PREDICTED AND ACTUAL RATINGS FOR EVALUATIONS \n",
    "pred_flatten = user_predictions[test_user_item_matrix.values.nonzero()].flatten()\n",
    "actual_flatten = test_user_item_matrix.values[test_user_item_matrix.values.nonzero()].flatten()\n",
    "\n",
    "#CALCULATE RMSE\n",
    "rmse = sqrt(mean_squared_error(pred_flatten, actual_flatten))\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38072208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a17cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf5bf958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations for User 1: [2, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nh013\\AppData\\Local\\Temp\\ipykernel_9536\\4074292167.py:108: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  ratings_diff = (train_user_item_matrix - mean_user_rating[:, np.newaxis])\n",
      "C:\\Users\\nh013\\AppData\\Local\\Temp\\ipykernel_9536\\4074292167.py:109: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  pred = mean_user_rating[:, np.newaxis] + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\Bulk Bookstore Dataset\\bulk_bookstore_dataset.csv\")\n",
    "\n",
    "# FUNCTION TO PERFORM NLP PREPROCESS\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # CONVERT TO LOWERCASE\n",
    "        text = text.lower()\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # STEMMING\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # LEMMATIZATION\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # JOIN TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''  \n",
    "\n",
    "# LIST OF COL CONTAIN TEXT DATA \n",
    "text_columns = ['Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author', 'Format', 'Publisher',\n",
    "                'Language', 'Weight', 'Dimensions', 'Raw description', 'Availability', \n",
    "                'Item condition', 'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id']\n",
    "\n",
    "#FUNCTION FOR PREPROCESS EACH COL\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "#COMBINE PREPROCESSED TEXT COL INTO A SINGLE COL \n",
    "df['combined_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRIX TO DATAFRAME \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE TF-IDF DATAFRAME WITH THE ORGINAL DATAFRAME \n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# CRAETE USER-ITEM MATRIXS\n",
    "user_item_matrix = df.drop(columns=['combined_text', 'Title', 'Url', 'Price', 'ISBN-10', 'ISBN', 'Author',\n",
    "                                    'Format', 'Publisher', 'Language', 'Weight', 'Dimensions',\n",
    "                                    'Raw description', 'Availability', 'Item condition', \n",
    "                                    'Breadcrumbs', 'Images', 'Scraped at', 'Uniq id'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# DATAFRAME\n",
    "df = pd.DataFrame({\n",
    "    'User_ID': [1, 1, 2, 2, 3, 3],\n",
    "    'Book_ID': [101, 102, 101, 103, 102, 103],\n",
    "    'Rating': [5, 4, 3, 5, 4, 2]\n",
    "})\n",
    "\n",
    "#SPLIT DATA INTO TRAIN AND TESTING SET\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#USER=ITEM MATRIX FOR TRAIN DATA\n",
    "train_user_item_matrix = train_data.pivot_table(index='User_ID', columns='Book_ID', values='Rating', fill_value=0)\n",
    "\n",
    "# CALCULATE SIMILARITY\n",
    "user_similarity = cosine_similarity(train_user_item_matrix)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "# PREDICT RATING FOR TEXT DATA \n",
    "def predict_ratings(user_similarity, train_user_item_matrix):\n",
    "    mean_user_rating = train_user_item_matrix.mean(axis=1)\n",
    "    ratings_diff = (train_user_item_matrix - mean_user_rating[:, np.newaxis])\n",
    "    pred = mean_user_rating[:, np.newaxis] + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "    return pred\n",
    "\n",
    "\n",
    "test_user_item_matrix = test_data.pivot_table(index='User_ID', columns='Book_ID', values='Rating', fill_value=0)\n",
    "\n",
    "#GET PRIDICTED RATING \n",
    "user_predictions = predict_ratings(user_similarity, train_user_item_matrix)\n",
    "\n",
    "# EVALUATE THE MODEL \n",
    "#FLATTEN THE PREDICTED AND ACTUAL RATINGS FOR EVALUATIONS \n",
    "pred_flatten = user_predictions[test_user_item_matrix.values.nonzero()].flatten()\n",
    "actual_flatten = test_user_item_matrix.values[test_user_item_matrix.values.nonzero()].flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FUNCTION TO GET TOP N RECOMMENDATION FOR EACH USERS\n",
    "def get_top_recommendations(user_predictions, n=5):\n",
    "    top_recommendations = {}\n",
    "    for user_id in user_predictions.index:\n",
    "        user_ratings = user_predictions.loc[user_id]\n",
    "        sorted_ratings = user_ratings.sort_values(ascending=False)\n",
    "        top_N_recommendations = sorted_ratings.head(n)\n",
    "        \n",
    "        # ADD 1 TO THE INDEX TO MAKE THEM NON ZERO BASED \n",
    "        top_recommendations[user_id] = top_N_recommendations.index + 1\n",
    "    return top_recommendations\n",
    "\n",
    "# GET TOP 5 RECOMMENDATIONS FOR EACH USERS \n",
    "top_recommendations = get_top_recommendations(pd.DataFrame(user_predictions))\n",
    "\n",
    "#DISPLAY RECOMMENDATION FOR A SPECIFIC USER\n",
    "user_id = 1\n",
    "print(f\"Top 5 recommendations for User {user_id}: {top_recommendations[user_id].tolist()}\")\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492ab76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
